{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Import the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas\n",
    "#%pip install matplotlib\n",
    "#%pip install seaborn\n",
    "#%pip install numpy\n",
    "#%pip install scikit-learn\n",
    "# %pip install pyodbc  \n",
    "# %pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Connect to the Microsoft SQL Server database and retrieve data for 2020 and 2021**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc      \n",
    "from dotenv import dotenv_values   \n",
    "import pandas as pd\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported the dotenv_values function from the dotenv package\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Loaded the environment variables from the .env file into a dictionary\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# I used the values for the credentials I set in the '.env' file\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "username = environment_variables.get(\"USERNAME\")\n",
    "password = environment_variables.get(\"PASSWORD\")\n",
    "\n",
    "# I then constructed the connection string for SQL Server\n",
    "connection_string = (\n",
    "    f\"DRIVER={{SQL Server}};SERVER={server};\"\n",
    "    f\"DATABASE={database};UID={username};PWD={password};MARS_Connection=yes\"\n",
    ")\n",
    "\n",
    "# After this, I used the 'connection_string' to connect to the SQL Server\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Load data for 2020 and 2021**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Select * from LP1_startup_funding2020\"\n",
    "\n",
    "df_2020 = pd.read_sql(query, connection)\n",
    "\n",
    "query = \"Select * from LP1_startup_funding2021\"\n",
    "\n",
    "df_2021 = pd.read_sql(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Load the data for 2019 from the OneDrive link**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We tried to load the dataset from the onedrive directly using the link, but unfortunately, what we tried gave us some errors so we used the link to get the CSV file root folder by downloading it from our web browser using the link provided and then moved the csv file to the Project Folder. From there, we used the file path method to locate the File and the loaded it using the pandas library.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# I specified the full file path\n",
    "file_path = \"C:/Users/IddieGod/Desktop/MY-LP1-PROJECT/startup_funding2019.csv\"\n",
    "\n",
    "# And then I used pandas to read the CSV file\n",
    "df_2019 = pd.read_csv(file_path)\n",
    "\n",
    "# Now, df_2019 contains the data from the 2019 CSV file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Load the data for 2018 from the GitHub repository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Provide the URL to the GitHub CSV file\n",
    "github_url = \"https://raw.githubusercontent.com/Azubi-Africa/Career_Accelerator_LP1-Data_Analysis/main/startup_funding2018.csv\"\n",
    "\n",
    "# Use pandas to read the CSV file\n",
    "df_2018 = pd.read_csv(github_url)\n",
    "\n",
    "# Now, df_2018 contains the data from the 2018 CSV file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Clean the data\n",
    "Cleaning the data involves handling missing values, removing duplicates, and standardizing the data format.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cleaning for df_2018*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in df_2018:\n",
      "Company Name     0\n",
      "Industry         0\n",
      "Round/Series     0\n",
      "Amount           0\n",
      "Location         0\n",
      "About Company    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in df_2018\n",
    "missing_values_2018 = df_2018.isnull().sum()\n",
    "\n",
    "print(\"Missing Values in df_2018:\")\n",
    "print(missing_values_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 526 entries, 0 to 525\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Company Name   526 non-null    object\n",
      " 1   Industry       526 non-null    object\n",
      " 2   Round/Series   526 non-null    object\n",
      " 3   Amount         526 non-null    object\n",
      " 4   Location       526 non-null    object\n",
      " 5   About Company  526 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 24.8+ KB\n",
      "None\n",
      "      Company Name                                           Industry  \\\n",
      "0  TheCollegeFever  Brand Marketing, Event Promotion, Marketing, S...   \n",
      "1  Happy Cow Dairy                               Agriculture, Farming   \n",
      "2       MyLoanCare   Credit, Financial Services, Lending, Marketplace   \n",
      "3      PayMe India                        Financial Services, FinTech   \n",
      "4         Eunimart                 E-Commerce Platforms, Retail, SaaS   \n",
      "\n",
      "  Round/Series       Amount                          Location  \\\n",
      "0         Seed       250000       Bangalore, Karnataka, India   \n",
      "1         Seed  ₹40,000,000        Mumbai, Maharashtra, India   \n",
      "2     Series A  ₹65,000,000           Gurgaon, Haryana, India   \n",
      "3        Angel      2000000       Noida, Uttar Pradesh, India   \n",
      "4         Seed            —  Hyderabad, Andhra Pradesh, India   \n",
      "\n",
      "                                       About Company  \n",
      "0  TheCollegeFever is a hub for fun, fiesta and f...  \n",
      "1  A startup which aggregates milk from dairy far...  \n",
      "2          Leading Online Loans Marketplace in India  \n",
      "3  PayMe India is an innovative FinTech organizat...  \n",
      "4  Eunimart is a one stop solution for merchants ...  \n"
     ]
    }
   ],
   "source": [
    "print(df_2018.info())\n",
    "print(df_2018.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows in df_2018:\n",
      "        Company Name                                           Industry  \\\n",
      "348  TheCollegeFever  Brand Marketing, Event Promotion, Marketing, S...   \n",
      "\n",
      "    Round/Series  Amount                     Location  \\\n",
      "348         Seed  250000  Bangalore, Karnataka, India   \n",
      "\n",
      "                                         About Company  \n",
      "348  TheCollegeFever is a hub for fun, fiesta and f...  \n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in df_2018\n",
    "duplicates_2018 = df_2018[df_2018.duplicated()]\n",
    "print(\"Duplicate rows in df_2018:\")\n",
    "print(duplicates_2018)\n",
    "\n",
    "# Remove duplicates in df_2018\n",
    "df_2018 = df_2018.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remove all non-numeric characters from the 'Amount' column\n",
    "df_2018['Amount'] = df_2018['Amount'].apply(lambda x: re.sub(r'[^0-9]', '', x))\n",
    "\n",
    "# Convert 'Amount' column to numeric\n",
    "df_2018['Amount'] = pd.to_numeric(df_2018['Amount'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code uses the re.sub function from the re module to remove any characters that are not digits (0-9) from each element in the 'Amount' column. It then converts the cleaned 'Amount' column to numeric. This approach ensures that all non-numeric characters, regardless of their type, are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 525 entries, 0 to 525\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Company Name   525 non-null    object \n",
      " 1   Industry       525 non-null    object \n",
      " 2   Round/Series   525 non-null    object \n",
      " 3   Amount         377 non-null    float64\n",
      " 4   Location       525 non-null    object \n",
      " 5   About Company  525 non-null    object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 28.7+ KB\n",
      "None\n",
      "      Company Name                                           Industry  \\\n",
      "0  TheCollegeFever  Brand Marketing, Event Promotion, Marketing, S...   \n",
      "1  Happy Cow Dairy                               Agriculture, Farming   \n",
      "2       MyLoanCare   Credit, Financial Services, Lending, Marketplace   \n",
      "3      PayMe India                        Financial Services, FinTech   \n",
      "4         Eunimart                 E-Commerce Platforms, Retail, SaaS   \n",
      "\n",
      "  Round/Series      Amount                          Location  \\\n",
      "0         Seed    250000.0       Bangalore, Karnataka, India   \n",
      "1         Seed  40000000.0        Mumbai, Maharashtra, India   \n",
      "2     Series A  65000000.0           Gurgaon, Haryana, India   \n",
      "3        Angel   2000000.0       Noida, Uttar Pradesh, India   \n",
      "4         Seed         NaN  Hyderabad, Andhra Pradesh, India   \n",
      "\n",
      "                                       About Company  \n",
      "0  TheCollegeFever is a hub for fun, fiesta and f...  \n",
      "1  A startup which aggregates milk from dairy far...  \n",
      "2          Leading Online Loans Marketplace in India  \n",
      "3  PayMe India is an innovative FinTech organizat...  \n",
      "4  Eunimart is a one stop solution for merchants ...  \n"
     ]
    }
   ],
   "source": [
    "print(df_2018.info())\n",
    "print(df_2018.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Amount column after convertion to numeric datatype now has missing values. To fill the missing data, we used the MICE (Multiple Imputation by Chained Equations) method which is a useful method for imputing missing values in multiple columns, especially when there is a complex relationship between the columns. We did not use fill.na of means/median as Mean imputation is sensitive to outliers and may not be a good representation of the central tendency of the data. Similarly to the mean, the median also may not better represent the central tendency.\n",
    "Median imputation makes the assumption that the data is missing completely at random (MCAR), which is not always true.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*cleaning the missing values in the amount column of df_2018*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\iddiegod\\appdata\\roaming\\python\\python312\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\iddiegod\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\iddiegod\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\iddiegod\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\iddiegod\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pandas as pd\n",
    "\n",
    "# List of numeric columns to impute\n",
    "numeric_columns = ['Amount']\n",
    "\n",
    "# Perform imputation on the numeric columns\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "numeric_imputed = imputer.fit_transform(df_2018[numeric_columns])\n",
    "\n",
    "# Convert the result back to a DataFrame with the same index\n",
    "numeric_imputed_df = pd.DataFrame(numeric_imputed, columns=numeric_columns, index=df_2018.index)\n",
    "\n",
    "# Combine the imputed numeric columns with the non-numeric columns\n",
    "df_2018_imputed = pd.concat([numeric_imputed_df, df_2018.drop(numeric_columns, axis=1)], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Imputation method\n",
    "\n",
    "\n",
    "1. **List of Numeric Columns**: We start by defining a list of column names that are numeric and need imputation. In this case, we only have one numeric column, which is 'Amount'.\n",
    "\n",
    "   ```python\n",
    "   numeric_columns = ['Amount']\n",
    "   ```\n",
    "\n",
    "2. **Imputation on Numeric Columns**:\n",
    "   - We create an `IterativeImputer` object to perform the imputation. It iteratively imputes missing values in numeric columns based on the values in other columns.\n",
    "   - `max_iter` specifies the maximum number of imputation iterations, and `random_state` ensures reproducibility.\n",
    "\n",
    "   ```python\n",
    "   imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "   ```\n",
    "\n",
    "3. **Perform Imputation**:\n",
    "   - We perform the imputation specifically on the numeric columns defined in the `numeric_columns` list. The imputer calculates missing values for the 'Amount' column.\n",
    "\n",
    "   ```python\n",
    "   numeric_imputed = imputer.fit_transform(df_2018[numeric_columns])\n",
    "   ```\n",
    "\n",
    "4. **Convert Imputed Numeric Values to DataFrame**:\n",
    "   - We convert the imputed numeric values back into a DataFrame. This new DataFrame retains the same index as the original DataFrame.\n",
    "\n",
    "   ```python\n",
    "   numeric_imputed_df = pd.DataFrame(numeric_imputed, columns=numeric_columns, index=df_2018.index)\n",
    "   ```\n",
    "\n",
    "5. **Combine Imputed Numeric and Non-Numeric Columns**:\n",
    "   - We combine the imputed numeric DataFrame (`numeric_imputed_df`) with the non-numeric columns. The non-numeric columns are obtained by dropping the 'Amount' column from the original DataFrame.\n",
    "\n",
    "   ```python\n",
    "   df_2018_imputed = pd.concat([numeric_imputed_df, df_2018.drop(numeric_columns, axis=1)], axis=1)\n",
    "   ```\n",
    "\n",
    "   This step ensures that `df_2018_imputed` contains both the imputed 'Amount' column and the other columns in their original string format.\n",
    "\n",
    "Now, you have a DataFrame (`df_2018_imputed`) with the imputed numeric column and all the other non-numeric columns, allowing you to proceed with your analysis while preserving the original data types for the non-numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in df_2018:\n",
      "Amount           0\n",
      "Company Name     0\n",
      "Industry         0\n",
      "Round/Series     0\n",
      "Location         0\n",
      "About Company    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in df_2018\n",
    "missing_values_2018 = df_2018_imputed.isnull().sum()\n",
    "\n",
    "print(\"Missing Values in df_2018:\")\n",
    "print(missing_values_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cleaning data for df_2019*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89 entries, 0 to 88\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Company/Brand  89 non-null     object \n",
      " 1   Founded        60 non-null     float64\n",
      " 2   HeadQuarter    70 non-null     object \n",
      " 3   Sector         84 non-null     object \n",
      " 4   What it does   89 non-null     object \n",
      " 5   Founders       86 non-null     object \n",
      " 6   Investor       89 non-null     object \n",
      " 7   Amount($)      89 non-null     object \n",
      " 8   Stage          43 non-null     object \n",
      "dtypes: float64(1), object(8)\n",
      "memory usage: 6.4+ KB\n",
      "None\n",
      "    Company/Brand  Founded HeadQuarter           Sector  \\\n",
      "0  Bombay Shaving      NaN         NaN        Ecommerce   \n",
      "1       Ruangguru   2014.0      Mumbai           Edtech   \n",
      "2        Eduisfun      NaN      Mumbai           Edtech   \n",
      "3        HomeLane   2014.0     Chennai  Interior design   \n",
      "4        Nu Genes   2004.0   Telangana         AgriTech   \n",
      "\n",
      "                                        What it does  \\\n",
      "0         Provides a range of male grooming products   \n",
      "1  A learning platform that provides topic-based ...   \n",
      "2            It aims to make learning fun via games.   \n",
      "3              Provides interior designing solutions   \n",
      "4  It is a seed company engaged in production, pr...   \n",
      "\n",
      "                                Founders  \\\n",
      "0                     Shantanu Deshpande   \n",
      "1  Adamas Belva Syah Devara, Iman Usman.   \n",
      "2                          Jatin Solanki   \n",
      "3           Srikanth Iyer, Rama Harinath   \n",
      "4                 Narayana Reddy Punyala   \n",
      "\n",
      "                                            Investor     Amount($)  \\\n",
      "0                               Sixth Sense Ventures    $6,300,000   \n",
      "1                                   General Atlantic  $150,000,000   \n",
      "2     Deepak Parekh, Amitabh Bachchan, Piyush Pandey   $28,000,000   \n",
      "3  Evolvence India Fund (EIF), Pidilite Group, FJ...   $30,000,000   \n",
      "4           Innovation in Food and Agriculture (IFA)    $6,000,000   \n",
      "\n",
      "           Stage  \n",
      "0            NaN  \n",
      "1       Series C  \n",
      "2  Fresh funding  \n",
      "3       Series D  \n",
      "4            NaN  \n"
     ]
    }
   ],
   "source": [
    "print(df_2019.info())\n",
    "print(df_2019.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       $6,300,000\n",
      "1     $150,000,000\n",
      "2      $28,000,000\n",
      "3      $30,000,000\n",
      "4       $6,000,000\n",
      "          ...     \n",
      "84     $20,000,000\n",
      "85    $693,000,000\n",
      "86      $5,000,000\n",
      "87     $50,000,000\n",
      "88     $33,000,000\n",
      "Name: Amount($), Length: 89, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Access and display the 'Amount' column in df_2019 \n",
    "#to check if there are non-numeric values within\n",
    "amount_column_2019 = df_2019['Amount($)']\n",
    "print(amount_column_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in df_2019:\n",
      "Company/Brand     0\n",
      "Founded          29\n",
      "HeadQuarter      19\n",
      "Sector            5\n",
      "What it does      0\n",
      "Founders          3\n",
      "Investor          0\n",
      "Amount($)         0\n",
      "Stage            46\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in df_2019\n",
    "missing_values_2019 = df_2019.isnull().sum()\n",
    "\n",
    "print(\"Missing Values in df_2019:\")\n",
    "print(missing_values_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Handling missing values in df_2019*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Handle missing values in 'HeadQuarter' column by filling with 'Unknown' or another appropriate value\n",
    "df_2019['HeadQuarter'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Handle missing values in 'Sector' column by filling with 'Unknown' or another appropriate category\n",
    "df_2019['Sector'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Handle missing values in 'Founders' column by filling with 'Unknown' or another appropriate value\n",
    "df_2019['Founders'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Handle missing values in 'Stage' column by filling with 'Unknown' or another appropriate category\n",
    "df_2019['Stage'].fillna('Unknown', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling missing values in 'Founded' column and replacing them with NaT(Not a Time)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Replace NaN values with pd.NaT to represent missing values\n",
    "df_2019['Founded'] = df_2019['Founded'].replace({np.nan: pd.NaT})\n",
    "\n",
    "# Convert 'Founded' to datetime (pd.NaT is recognized as a missing value)\n",
    "df_2019['Founded'] = pd.to_datetime(df_2019['Founded'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remove all non-numeric characters from the 'Amount' column in df_2019\n",
    "df_2019['Amount($)'] = df_2019['Amount($)'].apply(lambda x: re.sub(r'[^0-9]', '', x))\n",
    "\n",
    "# Convert 'Amount' column to numeric\n",
    "df_2019['Amount($)'] = pd.to_numeric(df_2019['Amount($)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in df_2019:\n",
      "Company/Brand     0\n",
      "Founded          29\n",
      "HeadQuarter       0\n",
      "Sector            0\n",
      "What it does      0\n",
      "Founders          0\n",
      "Investor          0\n",
      "Amount($)        12\n",
      "Stage             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verifying that missing values have been handled\n",
    "missing_values = df_2019.isnull().sum()\n",
    "print(\"Missing Values in df_2019:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                              NaT\n",
      "1    1970-01-01 00:00:00.000002014\n",
      "2                              NaT\n",
      "3    1970-01-01 00:00:00.000002014\n",
      "4    1970-01-01 00:00:00.000002004\n",
      "                  ...             \n",
      "84                             NaT\n",
      "85   1970-01-01 00:00:00.000002013\n",
      "86   1970-01-01 00:00:00.000002016\n",
      "87   1970-01-01 00:00:00.000002015\n",
      "88                             NaT\n",
      "Name: Founded, Length: 89, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "founded_column = df_2019['Founded']\n",
    "print(founded_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*checking for duplicates in df_2019*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows in df_2019:\n",
      "Empty DataFrame\n",
      "Columns: [Company/Brand, Founded, HeadQuarter, Sector, What it does, Founders, Investor, Amount($), Stage]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in df_2019\n",
    "duplicates_2019 = df_2019[df_2019.duplicated()]\n",
    "print(\"Duplicate rows in df_2019:\")\n",
    "print(duplicates_2019)\n",
    "\n",
    "# Remove duplicates in df_2019\n",
    "df_2019 = df_2019.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89 entries, 0 to 88\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Company/Brand  89 non-null     object        \n",
      " 1   Founded        60 non-null     datetime64[ns]\n",
      " 2   HeadQuarter    89 non-null     object        \n",
      " 3   Sector         89 non-null     object        \n",
      " 4   What it does   89 non-null     object        \n",
      " 5   Founders       89 non-null     object        \n",
      " 6   Investor       89 non-null     object        \n",
      " 7   Amount($)      77 non-null     float64       \n",
      " 8   Stage          89 non-null     object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(7)\n",
      "memory usage: 6.4+ KB\n",
      "None\n",
      "    Company/Brand                       Founded HeadQuarter           Sector  \\\n",
      "0  Bombay Shaving                           NaT     Unknown        Ecommerce   \n",
      "1       Ruangguru 1970-01-01 00:00:00.000002014      Mumbai           Edtech   \n",
      "2        Eduisfun                           NaT      Mumbai           Edtech   \n",
      "3        HomeLane 1970-01-01 00:00:00.000002014     Chennai  Interior design   \n",
      "4        Nu Genes 1970-01-01 00:00:00.000002004   Telangana         AgriTech   \n",
      "\n",
      "                                        What it does  \\\n",
      "0         Provides a range of male grooming products   \n",
      "1  A learning platform that provides topic-based ...   \n",
      "2            It aims to make learning fun via games.   \n",
      "3              Provides interior designing solutions   \n",
      "4  It is a seed company engaged in production, pr...   \n",
      "\n",
      "                                Founders  \\\n",
      "0                     Shantanu Deshpande   \n",
      "1  Adamas Belva Syah Devara, Iman Usman.   \n",
      "2                          Jatin Solanki   \n",
      "3           Srikanth Iyer, Rama Harinath   \n",
      "4                 Narayana Reddy Punyala   \n",
      "\n",
      "                                            Investor    Amount($)  \\\n",
      "0                               Sixth Sense Ventures    6300000.0   \n",
      "1                                   General Atlantic  150000000.0   \n",
      "2     Deepak Parekh, Amitabh Bachchan, Piyush Pandey   28000000.0   \n",
      "3  Evolvence India Fund (EIF), Pidilite Group, FJ...   30000000.0   \n",
      "4           Innovation in Food and Agriculture (IFA)    6000000.0   \n",
      "\n",
      "           Stage  \n",
      "0        Unknown  \n",
      "1       Series C  \n",
      "2  Fresh funding  \n",
      "3       Series D  \n",
      "4        Unknown  \n"
     ]
    }
   ],
   "source": [
    "print(df_2019.info())\n",
    "print(df_2019.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*cleaning the missing values in the amount column of df_2019 using MICE method*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pandas as pd\n",
    "\n",
    "# List of numeric columns to impute\n",
    "numeric_columns = ['Amount($)']  \n",
    "\n",
    "# Perform imputation on the numeric columns in df_2019\n",
    "imputer = IterativeImputer(max_iter=10, random_state=0)\n",
    "numeric_imputed = imputer.fit_transform(df_2019[numeric_columns])\n",
    "\n",
    "# Convert the result back to a DataFrame with the same index\n",
    "numeric_imputed_df = pd.DataFrame(numeric_imputed, columns=numeric_columns, index=df_2019.index)\n",
    "\n",
    "# Combine the imputed numeric columns with the non-numeric columns\n",
    "df_2019_imputed = pd.concat([numeric_imputed_df, df_2019.drop(numeric_columns, axis=1)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Amount($)    Company/Brand                       Founded HeadQuarter  \\\n",
      "0     6300000.0   Bombay Shaving                           NaT     Unknown   \n",
      "1   150000000.0        Ruangguru 1970-01-01 00:00:00.000002014      Mumbai   \n",
      "2    28000000.0         Eduisfun                           NaT      Mumbai   \n",
      "3    30000000.0         HomeLane 1970-01-01 00:00:00.000002014     Chennai   \n",
      "4     6000000.0         Nu Genes 1970-01-01 00:00:00.000002004   Telangana   \n",
      "..          ...              ...                           ...         ...   \n",
      "84   20000000.0     Infra.Market                           NaT      Mumbai   \n",
      "85  693000000.0              Oyo 1970-01-01 00:00:00.000002013    Gurugram   \n",
      "86    5000000.0       GoMechanic 1970-01-01 00:00:00.000002016       Delhi   \n",
      "87   50000000.0           Spinny 1970-01-01 00:00:00.000002015       Delhi   \n",
      "88   33000000.0  Ess Kay Fincorp                           NaT   Rajasthan   \n",
      "\n",
      "                     Sector  \\\n",
      "0                 Ecommerce   \n",
      "1                    Edtech   \n",
      "2                    Edtech   \n",
      "3           Interior design   \n",
      "4                  AgriTech   \n",
      "..                      ...   \n",
      "84                Infratech   \n",
      "85              Hospitality   \n",
      "86  Automobile & Technology   \n",
      "87               Automobile   \n",
      "88                  Banking   \n",
      "\n",
      "                                         What it does  \\\n",
      "0          Provides a range of male grooming products   \n",
      "1   A learning platform that provides topic-based ...   \n",
      "2             It aims to make learning fun via games.   \n",
      "3               Provides interior designing solutions   \n",
      "4   It is a seed company engaged in production, pr...   \n",
      "..                                                ...   \n",
      "84  It connects client requirements to their suppl...   \n",
      "85                Provides rooms for comfortable stay   \n",
      "86  Find automobile repair and maintenance service...   \n",
      "87                                Online car retailer   \n",
      "88              Organised Non-Banking Finance Company   \n",
      "\n",
      "                                             Founders  \\\n",
      "0                                  Shantanu Deshpande   \n",
      "1               Adamas Belva Syah Devara, Iman Usman.   \n",
      "2                                       Jatin Solanki   \n",
      "3                        Srikanth Iyer, Rama Harinath   \n",
      "4                              Narayana Reddy Punyala   \n",
      "..                                                ...   \n",
      "84                    Aaditya Sharda, Souvik Sengupta   \n",
      "85                                     Ritesh Agarwal   \n",
      "86  Amit Bhasin, Kushal Karwa, Nitin Rana, Rishabh...   \n",
      "87  Niraj Singh, Ramanshu Mahaur, Ganesh Pawar, Mo...   \n",
      "88                                     Rajendra Setia   \n",
      "\n",
      "                                             Investor          Stage  \n",
      "0                                Sixth Sense Ventures        Unknown  \n",
      "1                                    General Atlantic       Series C  \n",
      "2      Deepak Parekh, Amitabh Bachchan, Piyush Pandey  Fresh funding  \n",
      "3   Evolvence India Fund (EIF), Pidilite Group, FJ...       Series D  \n",
      "4            Innovation in Food and Agriculture (IFA)        Unknown  \n",
      "..                                                ...            ...  \n",
      "84  Tiger Global, Nexus Venture Partners, Accel Pa...       Series A  \n",
      "85  MyPreferred Transformation, Avendus Finance, S...        Unknown  \n",
      "86                                    Sequoia Capital       Series B  \n",
      "87  Norwest Venture Partners, General Catalyst, Fu...        Unknown  \n",
      "88     TPG, Norwest Venture Partners, Evolvence India        Unknown  \n",
      "\n",
      "[89 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_2019_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in df_2019:\n",
      "Amount($)         0\n",
      "Company/Brand     0\n",
      "Founded          29\n",
      "HeadQuarter       0\n",
      "Sector            0\n",
      "What it does      0\n",
      "Founders          0\n",
      "Investor          0\n",
      "Stage             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verifying that missing values have been handled\n",
    "missing_values = df_2019_imputed.isnull().sum()\n",
    "print(\"Missing Values in df_2019:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we viewed the 'Founded' column, you might have noticed that it still shows missing values represented as 'NaN' even after we filled the missing values with 'NaT' (Not a Timestamp). This may seem counterintuitive, but it's because 'NaT' is not recognized as a missing value in Pandas' datetime format. Instead, 'NaT' is used to indicate missing or undefined values in datetime columns.\n",
    "\n",
    "Even though Pandas displays 'NaT' as a special representation for missing datetime values when you view the column, it doesn't change the underlying missing value status of those entries. The column is still considered to have missing values, and it's displayed as such in the column's information.\n",
    "\n",
    "In other words, while 'NaN' is what you see, it's the 'NaT' values that are functioning as placeholders for missing dates in the 'Founded' column. This is the expected behavior when working with missing datetime values in Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cleaning data for df_2020*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
